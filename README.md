# Pipeline_databricks_azure

üîç Este projeto tem como objetivo automatizar a ingest√£o e transforma√ß√£o de dados utilizando Azure Data Factory e Databricks. Com a configura√ß√£o de um gatilho de execu√ß√£o por hora, o pipeline garante o processamento cont√≠nuo e eficiente de novos dados, promovendo maior agilidade e escalabilidade no fluxo de trabalho.

üöÄ Tecnologias Utilizadas

Azure Data Factory

Azure Databricks

GitHub

‚öôÔ∏è Instala√ß√£o e Configura√ß√£o

Siga os passos abaixo para configurar e executar o pipeline:

1- Crie um Grupo de Recursos no portal Azure para organizar todos os recursos relacionados ao projeto.

2- Crie uma Storage Account e, dentro dela, configure um Data Lake Gen2 para armazenar os dados de entrada, processamento e sa√≠da.

3- Registre uma aplica√ß√£o (App Registration) no Azure Active Directory para o Databricks, vinculando-a ao Grupo de Recursos criado. Essa etapa permite o controle de permiss√µes e autentica√ß√£o segura.

4- Crie um workspace no Azure Databricks dentro do mesmo Grupo de Recursos. Configure os notebooks respons√°veis pelo tratamento e transforma√ß√£o dos dados.

5- Configure o Azure Data Factory dentro do mesmo Grupo de Recursos e crie um pipeline de orquestra√ß√£o de dados.

6- Crie um gatilho (trigger) no Data Factory para executar o pipeline automaticamente a cada hora.

7- Estabele√ßa a conex√£o entre o Data Factory, o Data Lake e o Databricks, garantindo que os dados fluam corretamente entre os servi√ßos.

‚ñ∂Ô∏è Como Usar

Ap√≥s configurado, o pipeline ser√° executado automaticamente a cada hora ou quando novos dados forem disponibilizados na camada de entrada. O monitoramento das execu√ß√µes pode ser feito diretamente pelo portal do Azure, tanto no Data Factory quanto no Databricks.

ü§ù Contribui√ß√µes Contribui√ß√µes s√£o sempre bem-vindas! Se voc√™ tiver sugest√µes, melhorias ou corre√ß√µes, abra uma issue ou envie um pull request.

üìÑ Licen√ßa Este projeto est√° licenciado sob a Licen√ßa MIT.
